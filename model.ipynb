{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iURC6s25tBeo",
        "colab_type": "code",
        "outputId": "2677b309-f8a7-4232-d34a-1cfb336b1664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#setup env\n",
        "git clone https://github.com/tensorflow/privacy\n",
        "cd privacy\n",
        "pip install -e ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'privacy'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 636 (delta 32), reused 54 (delta 24), pack-reused 568\u001b[K\n",
            "Receiving objects: 100% (636/636), 264.10 KiB | 983.00 KiB/s, done.\n",
            "Resolving deltas: 100% (362/362), done.\n",
            "Obtaining file:///content/privacy\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from privacy==0.0.1) (1.2.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from privacy==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->privacy==0.0.1) (1.16.3)\n",
            "Installing collected packages: privacy\n",
            "  Running setup.py develop for privacy\n",
            "Successfully installed privacy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "v7c0AbFAuDd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3bw7LQy3vY5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "962a0bbd-7dc6-422f-9ebc-7c89852d9159"
      },
      "cell_type": "code",
      "source": [
        "import privacy\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NyrktlveTzlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2c059c3-a540-4092-cf62-75565ebe7490"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6c3p_04sTh27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv('./train.csv')\n",
        "train_labels = pd.read_csv('./clean_y.csv')\n",
        "test_data = pd.read_csv('test_x.csv')\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n",
        "train_labels = np.array(train_labels)\n",
        "train_data = train_data[:100000,1:]\n",
        "train_labels = train_labels[:100000,2]\n",
        "test_data = test_data[:100000,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXwFV1IO9FPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c3c7ac1-49c7-4f9e-e2f8-d8fc70092eac"
      },
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 0., ..., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "hBeuqnPaw3Vq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "27d0e6e3-0764-4db3-ed53-9fc548526c6c"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2019, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Training a CNN on MNIST with Keras and the DP SGD optimizer.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from privacy.analysis.rdp_accountant import compute_rdp\n",
        "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from privacy.dp_query.gaussian_query import GaussianAverageQuery\n",
        "from privacy.optimizers.dp_optimizer import DPGradientDescentOptimizer\n",
        "\n",
        "# Compatibility with tf 1 and 2 APIs\n",
        "try:\n",
        "  GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
        "except:  # pylint: disable=bare-except\n",
        "  GradientDescentOptimizer = tf.optimizers.SGD  # pylint: disable=invalid-name\n",
        "\n",
        "#tf.flags.DEFINE_boolean('dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
        "#                        'train with vanilla SGD.')\n",
        "#tf.flags.DEFINE_float('learning_rate', 0.15, 'Learning rate for training')\n",
        "#tf.flags.DEFINE_float('noise_multiplier', 1.1,\n",
        "#                      'Ratio of the standard deviation to the clipping norm')\n",
        "#tf.flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
        "#tf.flags.DEFINE_integer('batch_size', 250, 'Batch size')\n",
        "#tf.flags.DEFINE_integer('epochs', 60, 'Number of epochs')\n",
        "# tf.flags.DEFINE_integer('microbatches', 250, 'Number of microbatches '\n",
        "#                         '(must evenly divide batch_size)')\n",
        "#tf.flags.DEFINE_string('model_dir', None, 'Model directory')\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS.batch_size = 250\n",
        "FLAGS.learning_rate = 0.001\n",
        "def compute_epsilon(steps):\n",
        "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
        "  if FLAGS.noise_multiplier == 0.0:\n",
        "    return float('inf')\n",
        "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
        "  sampling_probability = FLAGS.batch_size / 100000\n",
        "  rdp = compute_rdp(q=sampling_probability,\n",
        "                    noise_multiplier=FLAGS.noise_multiplier,\n",
        "                    steps=steps,\n",
        "                    orders=orders)\n",
        "  # Delta is set to 1e-5 because MNIST has 100000 training points.\n",
        "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "\n",
        "\n",
        "\n",
        "def main(unused_argv):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  if FLAGS.dpsgd and FLAGS.batch_size % FLAGS.microbatches != 0:\n",
        "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
        "\n",
        "  # Define a sequential Keras model\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(64, activation='relu',input_shape=(207,)),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(2,activation = 'softmax')\n",
        "  ])\n",
        "\n",
        "  if FLAGS.dpsgd:\n",
        "    dp_average_query = GaussianAverageQuery(\n",
        "        FLAGS.l2_norm_clip,\n",
        "        FLAGS.l2_norm_clip * FLAGS.noise_multiplier,\n",
        "        FLAGS.microbatches)\n",
        "    optimizer = DPGradientDescentOptimizer(\n",
        "        dp_average_query,\n",
        "        FLAGS.microbatches,\n",
        "        learning_rate=FLAGS.learning_rate,\n",
        "        unroll_microbatches=True)\n",
        "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  else:\n",
        "    optimizer = GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  # Compile model with Keras\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  # Train model with Keras\n",
        "  model.fit(train_data, train_labels,\n",
        "            epochs=FLAGS.epochs,\n",
        "            batch_size=FLAGS.batch_size)\n",
        "  test_pred = model.predict(test_x)\n",
        "\n",
        "  # Compute the privacy budget expended.\n",
        "  if FLAGS.dpsgd:\n",
        "    eps = compute_epsilon(FLAGS.epochs * 100000 // FLAGS.batch_size)\n",
        "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
        "  else:\n",
        "    print('Trained with vanilla non-private SGD optimizer')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "100000/100000 [==============================] - 93s 930us/sample - loss: 0.6948 - acc: 1.0000\n",
            "Epoch 2/15\n",
            "100000/100000 [==============================] - 77s 768us/sample - loss: 0.6948 - acc: 1.0000\n",
            "Epoch 3/15\n",
            "100000/100000 [==============================] - 79s 785us/sample - loss: 0.6948 - acc: 1.0000\n",
            "Epoch 4/15\n",
            " 51250/100000 [==============>...............] - ETA: 38s - loss: 0.6936 - acc: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4VtPFt5HNrNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv('./train.csv')\n",
        "train_labels = pd.read_csv('./clean_y.csv')\n",
        "test_data = pd.read_csv('test_x.csv')\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n",
        "train_labels = np.array(train_labels)\n",
        "train_data = train_data[:100000,1:]\n",
        "train_labels = train_labels[:100000,1]\n",
        "test_data = test_data[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3K2lFeay6e5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2019, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Training a CNN on MNIST with Keras and the DP SGD optimizer.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from privacy.analysis.rdp_accountant import compute_rdp\n",
        "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from privacy.dp_query.gaussian_query import GaussianAverageQuery\n",
        "from privacy.optimizers.dp_optimizer import DPGradientDescentOptimizer\n",
        "\n",
        "# Compatibility with tf 1 and 2 APIs\n",
        "try:\n",
        "  GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
        "except:  # pylint: disable=bare-except\n",
        "  GradientDescentOptimizer = tf.optimizers.SGD  # pylint: disable=invalid-name\n",
        "\n",
        "#tf.flags.DEFINE_boolean('dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
        "#                        'train with vanilla SGD.')\n",
        "#tf.flags.DEFINE_float('learning_rate', 0.15, 'Learning rate for training')\n",
        "#tf.flags.DEFINE_float('noise_multiplier', 1.1,\n",
        "#                      'Ratio of the standard deviation to the clipping norm')\n",
        "#tf.flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
        "#tf.flags.DEFINE_integer('batch_size', 250, 'Batch size')\n",
        "#tf.flags.DEFINE_integer('epochs', 60, 'Number of epochs')\n",
        "# tf.flags.DEFINE_integer('microbatches', 250, 'Number of microbatches '\n",
        "#                         '(must evenly divide batch_size)')\n",
        "#tf.flags.DEFINE_string('model_dir', None, 'Model directory')\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS.batch_size = 250\n",
        "def compute_epsilon(steps):\n",
        "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
        "  if FLAGS.noise_multiplier == 0.0:\n",
        "    return float('inf')\n",
        "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
        "  sampling_probability = FLAGS.batch_size / 100000\n",
        "  rdp = compute_rdp(q=sampling_probability,\n",
        "                    noise_multiplier=FLAGS.noise_multiplier,\n",
        "                    steps=steps,\n",
        "                    orders=orders)\n",
        "  # Delta is set to 1e-5 because MNIST has 100000 training points.\n",
        "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "\n",
        "\n",
        "\n",
        "def main(unused_argv):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  if FLAGS.dpsgd and FLAGS.batch_size % FLAGS.microbatches != 0:\n",
        "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
        "\n",
        "  # Define a sequential Keras model\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(64, activation='relu',input_shape=(207,)),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  if FLAGS.dpsgd:\n",
        "    dp_average_query = GaussianAverageQuery(\n",
        "        FLAGS.l2_norm_clip,\n",
        "        FLAGS.l2_norm_clip * FLAGS.noise_multiplier,\n",
        "        FLAGS.microbatches)\n",
        "    optimizer = DPGradientDescentOptimizer(\n",
        "        dp_average_query,\n",
        "        FLAGS.microbatches,\n",
        "        learning_rate=FLAGS.learning_rate,\n",
        "        unroll_microbatches=True)\n",
        "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "    loss = tf.keras.losses.MeanSquaredError(\n",
        "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  else:\n",
        "    optimizer = GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  # Compile model with Keras\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  # Train model with Keras\n",
        "  model.fit(train_data, train_labels,\n",
        "            epochs=FLAGS.epochs,\n",
        "            batch_size=FLAGS.batch_size)\n",
        "  test_pred = model.predict(test_x)\n",
        "\n",
        "  # Compute the privacy budget expended.\n",
        "  if FLAGS.dpsgd:\n",
        "    eps = compute_epsilon(FLAGS.epochs * 100000 // FLAGS.batch_size)\n",
        "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
        "  else:\n",
        "    print('Trained with vanilla non-private SGD optimizer')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}